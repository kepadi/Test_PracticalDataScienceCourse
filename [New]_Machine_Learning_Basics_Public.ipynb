{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[New] Machine Learning Basics - Public.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OnnQZqXrZDMy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kepadi/Test_PracticalDataScienceCourse/blob/master/%5BNew%5D_Machine_Learning_Basics_Public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQde2o3Ftx_v"
      },
      "source": [
        "# Machine Learning Basics\n",
        "## Box Office Data\n",
        "\n",
        "Today, we will be exploring fundamental concepts of machine learning.\n",
        "\n",
        "__Question:__ How much gross revenue will a new movie make?\n",
        "\n",
        "__Data Set:__ Information from past movies and their gross revenue, including:\n",
        "- Actor and director information\n",
        "- Movie length \n",
        "- Budget\n",
        "\n",
        "__Goal:__ Use the data from the past to build a predictive model to help us predict how much revenue a movie will make\n",
        "\n",
        "\n",
        "You will need to add some code to complete this notebook.  Follow along with the instructor to find what code to add.  You will add that where the code says \"\\*\\*\\* ADD CODE HERE\\*\\*\\*\"\n",
        "\n",
        "Have fun and good luck coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hwgX0d-XCdR"
      },
      "source": [
        "> To execute a line or block of code, simply click the \"Play\" button on the left side or use the keyboard shortcut \"Shift + Enter\"\n",
        "> When that code block has actually been executed, the blank brackets will change to have a number inside of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkw0QgMyW-Ay"
      },
      "source": [
        "x = 42\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VSyLiB2jkXy"
      },
      "source": [
        "## Importing the packages that we'll need\n",
        "\n",
        "One of the things that makes Python **great** for data science is all of the different libraries that exist so we don't have to code them from scratch. Tonight we'll be taking advantage of:\n",
        "- [Numpy](https://numpy.org/) for scientific and mathematical computing\n",
        "- [Pandas](https://pandas.pydata.org/) for data wrangling and analysis\n",
        "- [Matplotlib](https://matplotlib.org/) for plotting and visualizations\n",
        "- [Sklearn](https://scikit-learn.org/stable/) for all things machine learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkkokDMOt221"
      },
      "source": [
        "# data analysis packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# plotting package\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL8FF6AExTtt"
      },
      "source": [
        "## Import the data\n",
        "Pandas can work with information from all kinds of data sources. Below, we'll import the data we need from a GitHub URL and read it into a Pandas Dataframe using the Pandas [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQusb-oRuC8I"
      },
      "source": [
        "# import data from github\n",
        "data = pd.read_csv(\"https://github.com/autumntoney/predict_the_box_office/raw/master/movie_metadata.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI9J_ZlDHezH"
      },
      "source": [
        "## Data Cleaning and Feature Engineering\n",
        "\n",
        "The data we just imported is not ready for modeling, so we will clean the data by removing duplicates and imputing or dropping missing values.  \n",
        "\n",
        "Then, using the cleaned data, we will engineer some features to hopefully improve our predictions.\n",
        "\n",
        "*Note: We won't get into this code in depth, but we wanted to provide the original data so you can go back to it if you wanted to clean it and/or engineer more features.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jknTusBVcY"
      },
      "source": [
        "# DATA CLEANING\n",
        "# Rename columns to include units\n",
        "data.rename(columns={'duration': 'duration_mins',\n",
        "                    'budget': 'budget_usd',\n",
        "                    'gross': 'gross_usd'}, inplace=True)\n",
        "\n",
        "# Drop all duplicate movie titles that were released in the same year\n",
        "data = data.drop_duplicates(subset=['movie_title', 'title_year'], keep='first').copy()\n",
        "\n",
        "# Drop the aspect ratio column,  axis=1 means drop the column\n",
        "data.drop('aspect_ratio', axis=1, inplace=True)\n",
        "\n",
        "# Drop all null values of gross\n",
        "# You will be predicting this value\n",
        "# so don't want to impute these values (skew the analysis)\n",
        "data.dropna(subset=['gross_usd'], how='all', inplace=True)\n",
        "\n",
        "# Drop movies where the year is missing\n",
        "data.dropna(subset=['title_year'], how='all', inplace=True)\n",
        "# Convert all years to integers\n",
        "data['title_year'] = data['title_year'].astype(int)\n",
        "\n",
        "# Calculate median budgets per year\n",
        "# Impute the median budgets per year for missing budget data\n",
        "data['budget_usd'] = data['budget_usd'].fillna(data.groupby('title_year')['budget_usd'].transform('median'))\n",
        "\n",
        "# Drop the remaining row that is missing budget\n",
        "# (no other movies from 1942 in the dataset)\n",
        "data.dropna(subset=['budget_usd'], axis=0, inplace=True)\n",
        "\n",
        "# Find how many movies are in each country in the data\n",
        "counts = data['country'].value_counts()\n",
        "# Select the data from only the top 3 countries\n",
        "data = data[data['country'].isin(counts.nlargest(3).index)].copy()\n",
        "\n",
        "# Dropping all remaining rows that have null values\n",
        "data.dropna(axis=0, inplace=True)\n",
        "\n",
        "# FEATURE ENGINEERING\n",
        "# Identify all movie counts, select all star actors\n",
        "lead_movie_counts = data['actor_1_name'].value_counts()\n",
        "star_actors = lead_movie_counts[lead_movie_counts >= 20].index\n",
        "# Set `lead_star` = 1 if actor is in star_actors, otherwise 0\n",
        "data['lead_star'] = [1 if x in star_actors else 0 for x in data['actor_1_name']]\n",
        "\n",
        "# Encoding ratings as dummy variables\n",
        "content_ratings = pd.get_dummies(data['content_rating'])\n",
        "# Merge the encoded data back on to the original data\n",
        "data = data.join(content_ratings)\n",
        "\n",
        "# Select columns by data type - number\n",
        "numerical_data = data.select_dtypes(include='number')\n",
        "numerical_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEyc_snwHPlt"
      },
      "source": [
        "## Selecting Features and Output\n",
        "We're just about ready for the machine learning phase of our project, but first we'll want to identify 2 different things:\n",
        "1. **Feature matrix**: this is the group of columns (features/variables/attributes) that will be used to determine our outcome.\n",
        "2. **Target array**: this is the series (target/label/outcome) that contains the information that we're ultimately trying to predict.\n",
        "\n",
        "So in this case, we're using the numerical data columns to predict the `gross_usd` revenue of the movies!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zvD-A3rWaxa"
      },
      "source": [
        "# Identifying our Feature set (X) and target (y) variables for modeling\n",
        "X = numerical_data.drop(['gross_usd'], axis=1)\n",
        "y = numerical_data['gross_usd']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ci7tGtolzas"
      },
      "source": [
        "Now that we have our data cleaned and features engineered, the final step before we can model is splitting the data into training sets and testing sets so we can evaluate the models.  We'll use the sklearn [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to do this, keeping 20% of the data in the test set, so we can train the model with 80% of the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1IIba5Mly0u"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6gBiaVjnSVU"
      },
      "source": [
        "## Machine Learning Steps\n",
        "We always do 4 things when modeling:\n",
        "1. Instantiate the model (start it up)\n",
        "2. Fit the model using the `.fit()` function\n",
        "3. Make predictions using the `.predict()` function\n",
        "4. Evaluate the model using our chosen evaluation metrics\n",
        "\n",
        "We will do all of these steps for each model we've chosen to use today, linear regression, decision tree, and random forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jktE61oFnCeC"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "A [linear regression](https://www.youtube.com/user/joshstarmer/search?query=linear+regression) model finds the line of best fit for the data.  \n",
        "\n",
        "This line of best fit has an equation similar to `y = mx+b`.  \n",
        "The line found by the model has a pre-determined `m` (slope), and `b` (intercept), so when you have a new input (`x`), your `y` is your prediction.\n",
        "\n",
        "Note that in this example, there is only one feature (`x`), but most models will use more features and therefore have more terms in the equation.\n",
        "\n",
        "[Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) models are good for simple data sets and where a linear relationship exists.\n",
        "\n",
        "\n",
        "*Note: Detail on [how much math is needed for Data Science](https://www.thinkful.com/blog/math-needed-for-data-science/)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTZ61kl5I70q"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://mlfromscratch.com/content/images/2020/01/linearRegression2-3.png\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKWhluI3Waki"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Step 1: Instantiating the model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Step 2: Fit the model (note, we use the train set here)\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwMXXH01pZ5O"
      },
      "source": [
        "Now that we've trained the model, we can make predictions.  We will predict the `y`-values of the test set using the `.predict()` function and plot the predictions against the actuals to determine how well the model predicted the gross revenue of the movie.  \n",
        "\n",
        "Note: If the model was perfect at predicting the gross revenue of movies, the points would follow the red line (`y=x`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Fzyo5FY4p_"
      },
      "source": [
        "# Step 3: Make predictions (note, we use the test set here)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Plot predicted values versus actual values\n",
        "plt.scatter(y_test, y_pred_lr)\n",
        "plt.plot([0,8e8], [0,8e8], linestyle='-', color='red')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Predicted and Actual Gross Revenue - Linear Regression');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPQZp2mOqFJv"
      },
      "source": [
        "We can see that the model did an okay job of predicting the gross revenues of movies that had a lower revenue, but it wasn't able to predict the output of the higher grossing movies.\n",
        "\n",
        "Let's evaluate our model using both R-squared ($R^2$) and [`Root`](https://numpy.org/doc/1.18/reference/generated/numpy.sqrt.html) [`Mean Squared Error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (RMSE).  As a reminder, models that can explain more variance in the data have $R^2$ values closer to 1, and lower RMSE means that the data is concentrated more closely around the line of best fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbXTYHHdWaCA"
      },
      "source": [
        "# Step 4: Evaluate the model (note, we use the test set here)\n",
        "print('Evaluating the Linear Regression Model')\n",
        "lr_r2 = lr.score(X_test, y_test)\n",
        "print('R Squared: \\t\\t\\t', lr_r2)\n",
        "\n",
        "lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "print('Root Mean Squared Error: \\t', lr_rmse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYm0-7LLNkiv"
      },
      "source": [
        "Great - we'll compare these evaluation metrics to the other models we are going to try. \n",
        "\n",
        "To help us understand how the model is making predictions, we can check out the Linear Regression's intercept and coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c8cCYwHooOB"
      },
      "source": [
        "# Interpreting how the model is making predictions\n",
        "# Linear regression models give the coefficients (m), and intercept (b)\n",
        "coefs = pd.DataFrame(zip(X.columns, lr.coef_), \n",
        "                     columns=['Feature Names', 'Coefficients'])\n",
        "print('Intercept: ', lr.intercept_)\n",
        "coefs.sort_values('Coefficients', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwArAu6s-tl"
      },
      "source": [
        "So what does this mean?\n",
        "\n",
        "The `intercept` is the gross revenue of a movie before you take into account any of the features of the movie.  Then each `coefficient` represents the amount of change in the gross revenue for each feature.  \n",
        "\n",
        "Linear Regression models are great for being able to explain exactly how the model made its predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnWZMxwAqOSU"
      },
      "source": [
        "## Decision Tree\n",
        "Decision Trees in classification problems are like flow charts where the model takes the data from the past to figure out the best split points to predict what category the data is in.  \n",
        "\n",
        "In regression, the model does the same, but instead of predicting the category, the numerical values at the end of the flow chart (leaves) are averaged to determine the predicted value.  \n",
        "\n",
        "In the example below, the `Windy = FALSE` number of hours played are all averaged to get the prediction value of 47.7 hours.\n",
        "And the `Windy = TRUE` hours played are averaged to get the prediction value of 26.5 hours.\n",
        "\n",
        "[Decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) are good for non-linear relationships, but can be prone to overfitting (the model predicts too closely to the training set).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBFj_pMtSDED"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://www.saedsayad.com/images/Decision_tree_r8.png\" width=\"700\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxZWzotqRWT"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Step 1: Instantiating the model\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Step 2: Fit the model (note, we use the train set here)\n",
        "dt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LikAuCDVqRJe"
      },
      "source": [
        "# Step 3: Make predictions (note, we use the test set here)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Plot predicted values versus actual values\n",
        "plt.scatter(y_test, y_pred_dt)\n",
        "plt.plot([0,8e8], [0,8e8], linestyle='-', color='red')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Predicted and Actual Gross Revenue - Decision Tree');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op9HX54cT4KB"
      },
      "source": [
        "Visually, this model was a little better at predicting the higher grossing movies than the linear regression model, but is worse at predicting the lower grossing movies.  Let's check out the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67x3bCzmqRQI"
      },
      "source": [
        "# Step 4: Evaluate the model (note, we use the test set here)\n",
        "print('Evaluating the Decision Tree Model')\n",
        "dt_r2 = dt.score(X_test, y_test)\n",
        "print('R Squared: \\t\\t\\t', dt_r2)\n",
        "dt_rmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
        "print('Root Mean Squared Error: \\t', dt_rmse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2TZWYGVVSXh"
      },
      "source": [
        "This Decision Tree has a lower $R^2$ and a higher RMSE than the linear regression, so our visual representation of the predictions matches with the evaluation metrics.\n",
        "\n",
        "Now for tree-based models, there aren't coefficients to explain the predictions, but they **do** have feature importances, which represent how much each feature (column/variable) impacts the predition.  Feature importances are values between 0 and 1, with higher values representing features that are more valuable in making predictions.  However, this does NOT represent if the feature is important in increasing or decreasing the predicted value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEDGHSd8qRUe"
      },
      "source": [
        "# Interpreting how the model is making predictions\n",
        "# Decision trees calculate the importance of features \n",
        "#(doesn't speak to directionality)\n",
        "import_dt = pd.DataFrame(zip(X.columns, dt.feature_importances_, ),\n",
        "                         columns=['Feature Names', 'Feature Importances'])\n",
        "import_dt.sort_values('Feature Importances', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8pr8-JktPep"
      },
      "source": [
        "This means that for this Decision Tree model the feature `num_voted_users` is the most important in predicting `gross_usd` revenue. And all of the features that have feature importances of 0 are not used to generate predictions for this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is4Gn9lXrJm8"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "A Random Forest model takes the great parts of the Decision Tree, and removes a lot of the risk of overfitting.  \n",
        "\n",
        "The model creates a bunch of Decision Tree predictions, and the resulting prediction is the average of all of the predictions for regression and the most often (mode) predictions for classification.  \n",
        "\n",
        "[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) models are good for complex datasets and when you have a large amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqIrrFEAncD2"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/2612/0*f_qQPFpdofWGLQqc.png\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPN1tEF5qQ-_"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 1: Instantiating the model\n",
        "***INSTANTIATE MODEL HERE***\n",
        "\n",
        "# Step 2: Fit the model (note, we use the train set here)\n",
        "***FIT MODEL ON TRAINING DATA HERE***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsp27CyUsFHl"
      },
      "source": [
        "# Step 3: Make predictions (note, we use the test set here)\n",
        "y_pred_rf = ***PREDICT VALUES ON TEST DATA HERE***\n",
        "\n",
        "# Plot predicted values versus actual values\n",
        "plt.scatter(y_test, y_pred_rf)\n",
        "plt.plot([0,8e8], [0,8e8], linestyle='-', color='red')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Predicted and Actual Gross Revenue - Random Forest');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsoDaUU7iJSQ"
      },
      "source": [
        "Again, the Random Forest model isn't good at predicting the higher grossing movies, but is fairly good at predicting the lower grossing movies.  Visually, it's hard to tell the difference between how the Random Forest and the Linear Regression model predicted `gross_usd`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkfrreQqsFbl"
      },
      "source": [
        "# Step 4: Evaluate the model (note, we use the test set here)\n",
        "print('Evaluating the Random Forest Model')\n",
        "rf_r2 = rf.score(X_test, y_test)\n",
        "print('R Squared: \\t\\t\\t', rf_r2)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "print('Root Mean Squared Error: \\t', rf_rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLVM-pCkZfH"
      },
      "source": [
        "This has a similar $R^2$ value as the Decision Tree, but a much lower RMSE. More on that in a minute.\n",
        "\n",
        "Random Forests are also tree-based, so use the same feature importances as Decision Trees to explain how they make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj_68aARqQ2O"
      },
      "source": [
        "# Interpreting how the model is making predictions\n",
        "# Decision trees calculate the importance of features \n",
        "#(doesn't speak to directionality)\n",
        "import_rf = pd.DataFrame(zip(X.columns, ***CALCULATE FEATURE IMPORTANCES HERE*** ), \n",
        "                         columns=['Feature Names', 'Feature Importances'])\n",
        "import_rf.sort_values('Feature Importances', ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcVzUdI_uTvP"
      },
      "source": [
        "Again, the `num_voted_users` feature is the most important in making predictions.  Note that the features that previously had feature importances of 0 are now no-zero because multiple Decision Trees are used in creating the Random Forest, so some trees might have used those features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpIhYPuklgIb"
      },
      "source": [
        "## Comparing the Models\n",
        "\n",
        "Now that we've trained three different models, let's compare their predictive ability so we can decide which model to use to make our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmf2ruzAuXy0"
      },
      "source": [
        "# Setting up our evaluation metrics so we can compare\n",
        "r2s = [lr_r2, dt_r2, rf_r2]\n",
        "rmses = [lr_rmse, dt_rmse, rf_rmse]\n",
        "model_names = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
        "# Putting all of the metrics together\n",
        "comparison = pd.DataFrame(zip(model_names, r2s, rmses),\n",
        "                           columns=['Model', 'R-squared Score', 'RMSE'])\n",
        "comparison"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-5-zDpsvlpv"
      },
      "source": [
        "The Random Forest model has the highest $R^2$ score, and the lowest RMSE, but we can only get feature importances, not exact coefficients from the model to explain how we got the predictions.\n",
        "\n",
        "**You be the judge:** Which model would you suggest to your boss that we should use to help make predictions about the gross revenue of a film? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WqRnPfz82dp"
      },
      "source": [
        "# Take Home Challenge\n",
        "- Drop the features that you can only determine AFTER the movie premiers, and see how that impacts your model's predictions\n",
        "- Try out different models for predicting the gross revenue of a movie \n",
        "  - Try [learning about kNN Regression](https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/) (predictions are the average of the neighbors), and seeing how [kNN Regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) works for this dataset. \n",
        "- Investigate outliers and clean or drop them and see how they impact the model predictions\n",
        "- Analyze your clean data (see Thinkful's Art of Visualization workshops/webinars for additional support)\n",
        "  - Drop features that are not related to `gross_usd` and see how it changes the model performance\n",
        "- Go back and impute some of the missing data that we dropped and see how that impacts the models\n",
        "- Continue to engineer features based on what you know about the subject and see how it impacts the models\n",
        "  - Maybe the era of the film might have something to do with the gross revenue.  The [history of film](http://www.historyoffilm.net/movie-eras/history-of-cinema/) documents the movie eras we can use to create our new feature along with Pandas [`.cut`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) function.\n",
        "  - Here is some starter code to get the ball rolling:\n",
        "  - Identify the era of the film\n",
        "\n",
        "    era_bins = [0, 1910, 1926, 1940, 1954, 1976, 2000, 2100]\n",
        "\n",
        "    era_labels = ['pioneer', 'silent', 'talkies', 'golden_era','changes', 'dawn_modern_film', 'modern_film']\n",
        "\n",
        "    data['era'] = pd.cut(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKI6yF3cuRZp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}